import os
import sys
import subprocess

# --- Adicionado para processamento ass√≠ncrono correto ---
import asyncio

# --- PROTOCOLO DE AUTO-INSTALA√á√ÉO DE EMERG√äNCIA ---
try:
    from llama_cpp import Llama
except ImportError:
    print("üì¶ [BRAIN]: Depend√™ncia 'llama-cpp-python' ausente. Iniciando auto-instala√ß√£o...")
    try:
        # Tenta for√ßar instala√ß√£o sem cache para evitar erro [Errno 13]
        subprocess.check_call([
            sys.executable, "-m", "pip", "install", "llama-cpp-python", 
            "--no-cache-dir", "--user", "--prefer-binary", "--upgrade"
        ])
        print("‚úÖ [BRAIN]: Instala√ß√£o conclu√≠da. Reiniciando importa√ß√£o...")
        from llama_cpp import Llama
    except Exception as e:
        print(f"‚ùå [ERRO CR√çTICO]: Falha ao instalar motor neural: {e}")
        Llama = None
# ----------------------------------------------------

class LocalLlamaBrain:
    def __init__(self, config=None):
        # 1. Calcula o caminho exato baseado onde este script est√°
        # features/local_brain.py -> sobe um n√≠vel -> pasta raiz -> models
        current_dir = os.path.dirname(os.path.abspath(__file__))
        root_dir = os.path.dirname(current_dir)
        
        # O caminho que voc√™ me passou
        self.model_path = os.path.join(root_dir, "models", "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf")
        
        self.n_ctx = 4096       # Mem√≥ria de contexto (Llama 3 aguenta at√© 8k)
        self.n_gpu_layers = 35  # Offload de 35 camadas para GPU. Use -1 para tudo, 0 para CPU.

        print(f"\nüß† [LOCAL AI]: Inicializando C√©rebro On-Premise...")
        print(f"üìÇ Alvo: {self.model_path}")

        if not os.path.exists(self.model_path):
            print(f"‚ùå ARQUIVO N√ÉO ENCONTRADO! Verifique se est√° em: {self.model_path}")
            self.llm = None
            return

        if Llama:
            try:
                # Carrega o modelo na RAM
                self.llm = Llama(
                    model_path=self.model_path,
                    n_ctx=self.n_ctx,
                    n_gpu_layers=self.n_gpu_layers,
                    verbose=False, # Reduz sujeira no log
                    chat_format="llama-3" # Formata automaticamente para o padr√£o do Llama 3
                )
                print(f"‚úÖ [LOCAL AI]: Llama-3 Operacional. Tentando offload de {self.n_gpu_layers} camadas para GPU.")
            except Exception as e:
                print(f"‚ùå [LOCAL AI]: Erro ao carregar na mem√≥ria: {e}")
                self.llm = None
        else:
            self.llm = None

    async def initialize(self):
        # M√©todo dummy para manter compatibilidade com a GUI antiga
        pass

    async def chat(self, role, prompt):
        """M√©todo simplificado para comandos diretos"""
        history = [{"role": "user", "content": prompt}]
        if role == "system":
            history = [{"role": "system", "content": prompt}]
        return await self.chat_complete(history)

    async def chat_complete(self, history):
        """
        Processa o hist√≥rico de chat e retorna a resposta de forma ass√≠ncrona.
        """
        if not self.llm:
            return self._criar_resposta_falsa("‚ö†Ô∏è Erro: C√©rebro Local n√£o foi carregado corretamente.")

        try:
            # A chamada para a LLM √© bloqueante. Usamos run_in_executor
            # para execut√°-la em uma thread separada e n√£o travar a GUI.
            loop = asyncio.get_running_loop()
            
            def blocking_llm_call():
                return self.llm.create_chat_completion(
                    messages=history,
                    temperature=0.7,
                    max_tokens=600,
                    stop=["<|eot_id|>", "Operador:", "R2:"]
                )

            output = await loop.run_in_executor(None, blocking_llm_call)
            
            texto_resposta = output['choices'][0]['message']['content']
            return self._criar_resposta_falsa(texto_resposta)

        except Exception as e:
            print(f"‚ùå Erro na infer√™ncia: {e}")
            return self._criar_resposta_falsa(f"‚ö†Ô∏è Falha no processamento neural: {e}")

    def _criar_resposta_falsa(self, texto):
        # Cria um objeto simples que tem o atributo .content (para a GUI n√£o quebrar)
        return type('obj', (object,), {'content': texto})